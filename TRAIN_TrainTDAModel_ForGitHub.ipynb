{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b6bab2-29c3-4038-b699-b76ffcaeb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile\n",
    "import cv2\n",
    "from os.path import join, isfile, exists\n",
    "import torch\n",
    "from torchview import draw_graph\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "import contextlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbc756e-582a-4e7e-9394-cfe7fb30b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=40\n",
    "n=40\n",
    "\n",
    "def strTmpt(t):\n",
    "    if t == 1:\n",
    "        return 't001'\n",
    "    elif t < 100:\n",
    "        return 't0'+str(t)\n",
    "    else:\n",
    "        return 't'+str(t)\n",
    "\n",
    "def prepl(t,a,b):\n",
    "    if t <= a or b<= t:\n",
    "        return 0\n",
    "    elif a < t and t <= (a+b)/2:\n",
    "        return t-a\n",
    "    else:\n",
    "        return b-t\n",
    "\n",
    "def prepl_list(t, intervals):\n",
    "    return [prepl(t,i[0],i[1]) for i in intervals]\n",
    "\n",
    "def prepare_Column(m, t, intervals):\n",
    "    pl_Vals = prepl_list(t, intervals)\n",
    "    pl_Vals = sorted(pl_Vals, reverse = True)\n",
    "    return pl_Vals[:m]\n",
    "\n",
    "def prepare_Input(filePath):\n",
    "    df1 = pd.read_csv(filePath)\n",
    "    startList1 = list(df1['starts'])\n",
    "    endList1 = list(df1['ends'])\n",
    "    l1 = [[startList1[i],endList1[i]] for i in range(len(startList1))]\n",
    "    return l1\n",
    "\n",
    "#returns the treatment condition for a particular sample\n",
    "#-samp is the sample name\n",
    "#-condFile is a .csv file with the sample names and conditions\n",
    "#each stored in separate columns\n",
    "#-condName is the name of the column of conditions in the file\n",
    "#-sampName is the name of the column of sample names in the file\n",
    "def get_condition(samp,condFile,condName = 'conds',sampName = 'samps'):\n",
    "    df = pd.read_csv(condFile)\n",
    "    return df[condName][list(df[sampName]).index(samp)]\n",
    "      \n",
    "#creates a dataset of labeled homology data (possibly of a particular dimension)\n",
    "#for a particular neural network, all from a single timepoint.\n",
    "#The initialization assumes the images are stored in folders according to \n",
    "#timepoint, and then labeled by sample\n",
    "#-homRoots is a list of this format:\n",
    "#     [path to 0-dim hom folders, path to 1-dim hom folders]\n",
    "#-time is the timepoint, given as 'txxx' (e.g. 't070' or 't210')\n",
    "#-condFile is the .csv file which has the treatment condition for each sample\n",
    "#-dims is the dimension of the homology we are using, either 0, 1, or both\n",
    "#-m is the number of persistence landscape functions\n",
    "#-n is the number of points to sample for persistence landscapes\n",
    "#   -width is the distance between sampled points\n",
    "#   -start is the starting sample point\n",
    "class HomDataset(Dataset):\n",
    "    def __init__(self,homRoots, time,condFile,dims, transform=None, m = 40, n = 40, width = 1, start=1):\n",
    "        root_dir = homRoots\n",
    "        self.transform = transform\n",
    "        self.pls = []\n",
    "        self.labels = []\n",
    "        label_map\n",
    "\n",
    "        if dims == 'Dim1' or dims == 'Dim0':\n",
    "            if dims == 'Dim0':\n",
    "                root_dir = homRoots[0]+strTmpt(t)\n",
    "            else:\n",
    "                root_dir = homRoots[1]+strTmpt(t)\n",
    "            files = [f for f in os.listdir(root_dir) if isfile(join(root_dir, f))]\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root_dir, file)\n",
    "                #these if-else statements are to address parsing the filename formats\n",
    "                if len(file[:file.find('.')]) == 6:\n",
    "                    label = get_condition(file[1:6],condFile)\n",
    "                else:\n",
    "                    label = get_condition(file[1:4]+'0'+file[4], condFile)\n",
    "                intvls = prepare_Input(file_path)\n",
    "                scalepoints = [start+i*width for i in range(n)]\n",
    "                plMat = np.asarray([prepare_Column(m, i, intvls) for i in scalepoints], dtype=np.float32)\n",
    "                label = label_map.get(label, -1)\n",
    "                if label != -1:\n",
    "                    self.pls.append(plMat)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "        else:\n",
    "            dir_one = homRoots[1]+strTmpt(t)\n",
    "            dir_zero = homRoots[0]+strTmpt(t)\n",
    "            files = [f for f in os.listdir(dir_one) if (isfile(join(dir_one, f)) and isfile(join(dir_zero,f)))]\n",
    "            for file in files:\n",
    "                path_one = os.path.join(dir_one, file)\n",
    "                path_zero = os.path.join(dir_zero, file)\n",
    "                #these if-else statements are to address parsing the filename formats\n",
    "                if len(file[:file.find('.')]) == 6:\n",
    "                    label = get_condition(file[1:6],condFile)\n",
    "                else:\n",
    "                    label = get_condition(file[1:4]+'0'+file[4], condFile)\n",
    "\n",
    "                intvls_one = prepare_Input(path_one)\n",
    "                intvls_zero = prepare_Input(path_zero)\n",
    "                scalepoints = [start+i*width for i in range(n)]\n",
    "                plMat_one = np.asarray([prepare_Column(int(m/2), i, intvls_one) for i in scalepoints], dtype=np.float32)\n",
    "                plMat_zero = np.asarray([prepare_Column(int(m/2), i, intvls_zero) for i in scalepoints], dtype=np.float32)\n",
    "                label = label_map.get(label, -1)\n",
    "                plMat = np.hstack((plMat_zero,plMat_one))\n",
    "                if label != -1:\n",
    "                    self.pls.append(plMat)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pls)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pl = self.pls[idx]\n",
    "        label = self.labels[idx]\n",
    "        # apply transformation         \n",
    "        if self.transform is not None:\n",
    "            pl = self.transform(pl)\n",
    "        return pl, label\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=100, debug=False):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_values = []\n",
    "    for epoch in range(num_epochs+1):\n",
    "        if epoch % 5 == 0 and debug:\n",
    "            print(f'Epoch {epoch}/{num_epochs}')\n",
    "            \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            loss_values.append(epoch_loss)\n",
    "            epoch_acc =  running_corrects / total\n",
    "            if epoch % 5 == 0 and debug:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')   \n",
    "                \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if epoch % 5 == 0 and debug:  \n",
    "            print('-' * 10)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_values\n",
    "\n",
    "def get_metrics(model, test_dataloader):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    accuracy_values = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "label_map = {\"BMP4\" :0, \"CHIR\": 1, \"DS\": 2, \"DS+CHIR\": 3,  \"WT\": 4}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TDANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.runthru = nn.Sequential(\n",
    "            nn.Linear(m*n, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20,5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.runthru(x)\n",
    "        return logits    \n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#-homRoots is a list of this format:\n",
    "#     [path to 0-dim hom folders, path to 1-dim hom folders]\n",
    "#-time is the timepoint at which we are training/testing\n",
    "#-condFile is the .csv file which has the treatment condition for each sample\n",
    "#-dims is the dimension of the homology we are using, either 0, 1, or both\n",
    "#-landFunct is the number of persistence landscape functions\n",
    "#-train is whether we are training a model, or justing testing an already existing one\n",
    "def get_dataloaders(homRoots,time, condFile, dims, landFunct = 40, train = True):\n",
    "    dataset = HomDataset(homRoots,time,condFile,dims, m = landFunct, transform=transform)\n",
    "    if train:\n",
    "        train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "        dataloaders = {'train': train_dataloader, 'val': test_dataloader}\n",
    "        return dataloaders\n",
    "    else:\n",
    "        test_dataloader = DataLoader(dataset, batch_size=8,shuffle = False)\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104748f6-25d1-4d35-82d7-396852fd7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-homRoots is a list of this format:\n",
    "#     [path to 0-dim hom folders, path to 1-dim hom folders]\n",
    "#-time is the timepoint at which we are training/testing\n",
    "#-condFileTr is the .csv file which has the treatment condition for each sample (for training)\n",
    "#-condFileTe is the .csv file which has the treatment condition for each sample (for testing)\n",
    "#    the different condition file names are for cases like say, you want to train the model\n",
    "#    on randomized labels and test on the actual labels, as a control\n",
    "#-dims is the dimension of the homology we are using, either 0, 1, or both\n",
    "#-ldfs is the number of persistence landscape functions\n",
    "#-train is whether we are training a model, or justing testing an already existing one\n",
    "#-saveModel is whether to save the model\n",
    "#-modelName is the filename of the model if saved\n",
    "def train_at_Tmpt(homRoots,t,condFileTr, condFileTe, dims,ldfs,saveModel = 1, modelName = 'MyTDANetModel.pt'):\n",
    "    tOI = strTmpt(t)\n",
    "    model = TDANet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    model = model.to(device)\n",
    "    original_stderr = sys.stderr\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    dataloaders = get_dataloaders(homRoots,t,condFileTr, dims, landFunct = ldfs)\n",
    "    model_trained, loss_values = train_model(model, criterion, optimizer, dataloaders, num_epochs=50)\n",
    "    dataloaders_actual = get_dataloaders(homRoots,t,condFileTe, dims, landFunct = ldfs)\n",
    "    accuracy, precision, recall, fOne = get_metrics(model_trained, dataloaders_actual['val'])\n",
    "    if saveModel==1:\n",
    "        torch.save(model_trained, modelName)\n",
    "    sys.stderr = original_stderr\n",
    "    timestamp:  1\n",
    "    return accuracy, precision, recall, fOne, model_trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
